{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HRFlow - Job API.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cDKMiYuSL13j"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDKMiYuSL13j",
        "colab_type": "text"
      },
      "source": [
        "##### Copyright 2020 HrFlow's AI Research Department\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzSEKxerPZN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copyright 2020 HrFlow's AI Research Department. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdr1gCzy-FVp",
        "colab_type": "text"
      },
      "source": [
        "# Job API:\n",
        "\n",
        "This notebook illustrates how to use **HrFlow's Job API**. This API serves as an interface to upload jobs (either structured as a json or as a file located in your hard drive) and retrieve results from HrFlow. In the current version, the following results can be retrieved or used:\n",
        "* The **Job object** \n",
        "* The **Searchin engine** and **Scoring**\n",
        "* The **embeddings** at various degree of granularity\n",
        "\n",
        "An **example of applications** with the Job API is available below. The example shows how **embeddings** can be leveraged to **classify jobs**. \n",
        "\n",
        "**Embeddings** eases the management of documents like resumes of jobs. It turns any highly structured image of a resume into a single **vector of numbers** with fixed length. \n",
        "\n",
        "The document embeddings can also be trivially used to compute **job or profile level meaning similarity** as well as to enable better performance on downstream classification tasks using **less supervised training data**.\n",
        "\n",
        "\n",
        "**Disclaimer**: Jobs comes from [pole-emploi.fr](https://www.pole-emploi.fr) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOrPy43XIn1P",
        "colab_type": "text"
      },
      "source": [
        "<p>\n",
        "<table align=\"left\"><td>\n",
        "  <a target=\"_blank\"  href=\"https://colab.research.google.com/github/Riminder/python-hrflow-api/blob/master/examples/colab/hrflow_job_api.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab\n",
        "  </a>\n",
        "</td><td>\n",
        "  <a target=\"_blank\"  href=\"https://github.com/Riminder/python-hrflow-api/blob/master/examples/colab/hrflow_job_api.ipynb\">\n",
        "    <img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "</td><td>\n",
        "  <a target=\"_blank\"  href=\"https://www.hrflow.ai/book-us\">\n",
        "    <img width=32px src=\"https://gblobscdn.gitbook.com/spaces%2F-M1L6Hspq8r9LXd5_gIC%2Favatar-1586188377926.png?generation=1586188378327930&alt=media\" />Get an account</a>\n",
        "</td></table>\n",
        "<br>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kasMnMod98Ud",
        "colab_type": "text"
      },
      "source": [
        "# Getting Started\n",
        "This section sets up the environment to get access to **HrFlow Job API** and sets up a connection to HrFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b64rhDasaCOp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Machine Learning and Classification Libs\n",
        "!pip install --quiet tensorflow\n",
        "!pip install --quiet matplotlib\n",
        "!pip install --quiet pandas\n",
        "!pip install --quiet seaborn\n",
        "!pip install --quiet plotly\n",
        "\n",
        "# HrFlow Dependencies\n",
        "!apt-get install libmagic-dev\n",
        "!pip install --quiet python-magic\n",
        "!pip install --quiet hrflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aBYeCXRau09",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "ROOT_PATH = \"drive/My Drive/Data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UblakiK4KJW",
        "colab_type": "text"
      },
      "source": [
        "An **API Key** is required here. You can get your API Key at **https://```<your-sub domain/>```.hrflow.ai/settings/api/keys** or ask us for a **demo API Key**.\n",
        "\n",
        "Either add your API Key as a file in your 'ROOT_PATH' or set the python variable named api_secret to your  API Key (api_secret = 'YOUR_SECRET_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiG13iIYR0ej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pprint\n",
        "import hrflow as hf\n",
        "\n",
        "# with open(os.path.join(ROOT_PATH,'api_key'), 'rb') as file:\n",
        "#   api_secret = pickle.load(file)\n",
        "\n",
        "client = hf.Client(api_secret=\"ask_0dec90609f229d31d0bdd6a03da4f588\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTTPiQ6pOZes",
        "colab_type": "text"
      },
      "source": [
        "# 1. Job API Routes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPB0Jat74Xh0",
        "colab_type": "text"
      },
      "source": [
        "There is currently 5 routes for this API:\n",
        "*  **Indexing**: Uploading a job by specifying some informations. The job will be stored as a JSON file in HrFlow\n",
        "*  **Parsing**: Retrieve job's parsing.\n",
        "*  **Embedding**: Retrieve your jobs **Embeddings** to build your custom solution\n",
        "*  **searching** Retrieve a list of Job based on filters\n",
        "*  **scoring** Retrieve a list of Job based on filters and their scores for a given profile."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA5SNUw41KoX",
        "colab_type": "text"
      },
      "source": [
        "## 1.1. Upload Json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gl_-MXOZT3C9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "job_json = {\n",
        "    \"name\": \"Data Engineer\",\n",
        "    \"agent_key\": \"cb0a59170cf0034a2fe5912382cdc478bc001ecc\",\n",
        "    \"reference\": \"Job's reference 07082020\",\n",
        "    \"url\": \"https://www.pole-emploi.ai/jobs/data_engineer\",\n",
        "    \"summary\": \"As an engineer for the Data Engineering Infrastructure team, you will design, build, scale, and evolve our data engineering  platform, services and tooling. Your work will have a critical  impact on all areas of business:supporting detailed internal analytics, calculating customer usage, securing our platform, and much more.\",\n",
        "    \"location\": {\n",
        "                  \"text\": \"Dampierre en Burly (45)\",\n",
        "                  \"geopoint\": {\n",
        "                      \"lat\": 47.7667,\n",
        "                      \"lon\": 2.5167\n",
        "                  }\n",
        "                 },\n",
        "    \"sections\": [{\n",
        "                    \"name\": \"profile\",\n",
        "                    \"title\": \"Searched Profile\",\n",
        "                    \"description\": \"Bac+5\"\n",
        "                  }\n",
        "                  ],\n",
        "    \"skills\": [{\n",
        "                  \"name\": \"python\",\n",
        "                  \"value\": None\n",
        "               },\n",
        "               {\n",
        "                  \"name\": \"spark\",\n",
        "                  \"value\": 0.9\n",
        "               }\n",
        "               ],\n",
        "    \"languages\": [{\n",
        "                     \"name\": \"english\",\n",
        "                     \"value\": 1\n",
        "                  },\n",
        "                 {  \n",
        "                     \"name\": \"french\",\n",
        "                     \"value\": 1\n",
        "                  }\n",
        "                  ],\n",
        "    \"tags\": [{\n",
        "                \"name\": \"archive\",\n",
        "                \"value\": True\n",
        "             },\n",
        "             {  \n",
        "                \"name\": \"tag example\",\n",
        "                \"value\": \"tag\"\n",
        "              }\n",
        "              ],\n",
        "    \"ranges_date\": [{\n",
        "                       \"name\": \"Dates\",\n",
        "                       \"value_min\": \"2020-05-18T21:59\",\n",
        "                       \"value_max\": \"2020-09-15T21:59\"\n",
        "                    }\n",
        "                    ],\n",
        "    \"ranges_float\": [{\n",
        "                       \"name\": \"salary\",\n",
        "                       \"value_min\": 30,\n",
        "                       \"value_max\": 40,\n",
        "                       \"unit\": \"eur\"\n",
        "                    }\n",
        "                    ],\n",
        "    \"metadatas\": [{\n",
        "                     \"name\": \"metadata example\",\n",
        "                     \"value\": \"metadata\"\n",
        "                  }\n",
        "                  ]\n",
        "}\n",
        "\n",
        "response = client.job.indexing.add_json(board_key=\"board_key\", job_json=job_json)\n",
        "pprint.pprint(response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZDCvxp0Ferr",
        "colab_type": "text"
      },
      "source": [
        "## 1.2. Edit Existing job"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FL7sFArwFjdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "job_json = {\n",
        "    \"name\": \"Data Engineer\",\n",
        "    \"agent_key\": \"cb0a59170cf0034a2fe5912382cdc478bc001ecc\",\n",
        "    \"reference\": \"Job's reference abc\",\n",
        "    \"url\": \"https://www.pole-emploi.ai/jobs/data_engineer\",\n",
        "    \"summary\": \"As an engineer for the Data Engineering Infrastructure team, you will design, build, scale, and evolve our data engineering  platform, services and tooling. Your work will have a critical  impact on all areas of business:supporting detailed internal analytics, calculating customer usage, securing our platform, and much more.\",\n",
        "    \"location\": {\n",
        "                  \"text\": \"Dampierre en Burly (45)\",\n",
        "                  \"geopoint\": {\n",
        "                      \"lat\": 47.7667,\n",
        "                      \"lon\": 2.5167\n",
        "                  }\n",
        "                 },\n",
        "    \"sections\": [{\n",
        "                    \"name\": \"profile\",\n",
        "                    \"title\": \"Searched Profile\",\n",
        "                    \"description\": \"Bac+5\"\n",
        "                  }\n",
        "                  ],\n",
        "    \"skills\": [{\n",
        "                  \"name\": \"python\",\n",
        "                  \"value\": None\n",
        "               },\n",
        "               {\n",
        "                  \"name\": \"spark\",\n",
        "                  \"value\": 0.9\n",
        "               }\n",
        "               ],\n",
        "    \"languages\": [{\n",
        "                     \"name\": \"english\",\n",
        "                     \"value\": 1\n",
        "                  },\n",
        "                 {  \n",
        "                     \"name\": \"french\",\n",
        "                     \"value\": 1\n",
        "                  }\n",
        "                  ],\n",
        "    \"tags\": [{\n",
        "                \"name\": \"archive\",\n",
        "                \"value\": False\n",
        "             },\n",
        "             {  \n",
        "                \"name\": \"tag example\",\n",
        "                \"value\": \"tag\"\n",
        "              }\n",
        "              ],\n",
        "    \"ranges_date\": [{\n",
        "                       \"name\": \"Dates\",\n",
        "                       \"value_min\": \"2020-05-18T21:59\",\n",
        "                       \"value_max\": \"2020-09-15T21:59\"\n",
        "                    }\n",
        "                    ],\n",
        "    \"ranges_float\": [{\n",
        "                       \"name\": \"salary\",\n",
        "                       \"value_min\": 30,\n",
        "                       \"value_max\": 50,\n",
        "                       \"unit\": \"eur\"\n",
        "                    }\n",
        "                    ],\n",
        "    \"metadatas\": [{\n",
        "                     \"name\": \"metadata example\",\n",
        "                     \"value\": \"metadata\"\n",
        "                  }\n",
        "                  ]\n",
        "}\n",
        "\n",
        "response = client.job.indexing.edit(board_key=\"board_key\", key=\"job_key\",job_json=job_json)\n",
        "pprint.pprint(response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8WQsQzC1U1H",
        "colab_type": "text"
      },
      "source": [
        "## 1.3. Get Job object"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_dhcw0V771V",
        "colab_type": "text"
      },
      "source": [
        "The method client.job.indexing.get retrieves the informations of a given job. It uses two mandatory fields:  **board_key** and **job_key** or **job_reference**. The job_key is returned as part of a response's upload : **key**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RnOgIU4oH9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "response = client.job.indexing.get(board_key=\"board_key\", key=\"job_key\")\n",
        "pprint.pprint(response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zJYJxvm1doM",
        "colab_type": "text"
      },
      "source": [
        "## 1.4. Embeddings Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kR3d3oyK56FP",
        "colab_type": "text"
      },
      "source": [
        "client.job.embedding.get returns embeddings for a given job (uniquely defined by the pair **board_key** and **job_key** or **job_reference**). \n",
        "\n",
        "This methods presently handles only job per job embeddings. A loop is required to get embeddings for more than one job."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uX926E6d4ePk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "response = client.job.embedding.get(board_key=\"board_key\", key=\"job_key\")\n",
        "pprint.pprint(response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y77PGkh9IN4C"
      },
      "source": [
        "## 1.5. Advanced Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Eotk8ZqpIc8_"
      },
      "source": [
        "### 1.5.1. Job Search Engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoA0WoliI0gz",
        "colab_type": "text"
      },
      "source": [
        "client.job.searching searches jobs based on their **name**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CqXK6kqHFT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "response = client.job.searching.list(board_keys=[\"board_key\"], page=1, limit=30, sort_by='created_at')\n",
        "pprint.pprint(response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9gLqoWNSkyu",
        "colab_type": "text"
      },
      "source": [
        "### 1.5.2. Job Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIF-LNcOJHT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "response  = client.job.scoring.list(board_keys=[\"board_key\"],\n",
        "                                  source_key=\"source_key\",\n",
        "                                  profile_key=\"profile_key\",\n",
        "                                  use_agent=1,\n",
        "                                  agent_key=\"agent_key\",\n",
        "                                  page=1, limit=30, sort_by='created_at', order_by=\"desc\")\n",
        "pprint.pprint(response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PvdAkFy8v9q",
        "colab_type": "text"
      },
      "source": [
        "# 2. Machine Learning With Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DssHdA6X6KHm",
        "colab_type": "text"
      },
      "source": [
        "Embeddings is widely used these last years (2013 onwards) in the field of *Natural Language Processing*, thanks to Tomas Mikolov and his team at Google. Their breakthrough on building reliable embeddings for words had a huge impact both scientifically and technologically.\n",
        "\n",
        "The rough idea behind embeddings consists in **numerically capture the meaning or informations** of a word (or sentence or even a whole document like a resume). Any resume can thus be relatively accurately represented by a set of real numbers ('vector of floats'). The **measure of accuracy** is evaluated to a predefined task. \n",
        "\n",
        "An embedding algorithm is deemed to be 'good' as for being good for a given **evaluation task**. In the case of word embeddings, the latter can be trained and evaluated (on the same task) on filling sentences gaps. This task quantifies how an embedding algorithm performs at knowing a sentence context (sequence words in the sentence) by filling missing words.\n",
        "\n",
        "In our case, the most obvious, practical and meaningfull evaluation task is the **classification of jobs** (which ones are 'bakers', 'data scientists', etc). This task is usually quite easily done by humans (Human Resources departments) and relatively well done by computers (keywords).\n",
        "\n",
        "The following cells of this notebook shows a relatively simple model that classifies some type of jobs based on pole emploi jobs (HrFlow Crawling Pipeline Feature). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ws1-a0IbKLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "def load_embedding(url):\n",
        "  response = requests.get(url, stream=True)\n",
        "  with open('tmp', 'wb') as file:\n",
        "      shutil.copyfileobj(response.raw, file)\n",
        "  return np.load('tmp', allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhDjDyUXOjmg",
        "colab_type": "text"
      },
      "source": [
        "## 2.1. Embeddings Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGeEXSM-TKfj",
        "colab_type": "text"
      },
      "source": [
        "We advise, in the case of retrieving a great amount of embeddings, to get embeddings asynchronously.\n",
        "\n",
        "The next colab cell download some embeddings knowing a list of **job_id**. Extra information for the given jobs have been saved in a file named **job_types** (the pole emploi job_type associated to the job_id). The embeddings, the job description and title and the job types are saved to the hard disk for later usages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FowbE2JCT32Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading From HrFlow\n",
        "from tqdm import tqdm\n",
        "\n",
        "with open(os.path.join(ROOT_PATH, 'jobs_ids'), 'rb') as file:\n",
        "  jobs_ids = pickle.load(file)\n",
        "with open(os.path.join(ROOT_PATH, 'jobs_types'), 'rb') as file:\n",
        "  jobs_types = pickle.load(file)\n",
        "jobs_texts = []\n",
        "jobs_embeddings = []\n",
        "\n",
        "for job_id in tqdm(jobs_ids):\n",
        "  # Get Embedding\n",
        "  response = client.job.embedding.get(job_id=job_id)\n",
        "  jobs_embeddings.append(load_embedding(response['data']))\n",
        "  # Get Job Informations\n",
        "  response = client.job.parsing.get(job_id=job_id)\n",
        "  job_text = {'title': response['data']['name'], \n",
        "              'description': response['data']['description']}\n",
        "  jobs_texts.append(job_text)\n",
        "\n",
        "# Save Data To Disk\n",
        "with open(os.path.join(ROOT_PATH, 'jobs_embeddings'), 'wb') as file:\n",
        "  pickle.dump(jobs_embeddings, file) \n",
        "with open(os.path.join(ROOT_PATH, 'jobs_texts'), 'wb') as file:\n",
        "  pickle.dump(jobs_texts, file) \n",
        "with open(os.path.join(ROOT_PATH, 'jobs_types'), 'wb') as file:\n",
        "  pickle.dump(jobs_types, file) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4no-XRW8QgeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading From Disk\n",
        "with open(os.path.join(ROOT_PATH, 'jobs_embeddings'), 'rb') as file:\n",
        "  jobs_embeddings = pickle.load(file) \n",
        "with open(os.path.join(ROOT_PATH, 'jobs_texts'), 'rb') as file:\n",
        "  jobs_texts = pickle.load(file) \n",
        "with open(os.path.join(ROOT_PATH, 'jobs_types'), 'rb') as file:\n",
        "  jobs_types = pickle.load(file) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LWpj01GOmxC",
        "colab_type": "text"
      },
      "source": [
        "## 2.2. Job Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J15puhh5j_rA",
        "colab_type": "text"
      },
      "source": [
        "#### 2.2.a. Model: Shallow Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzO0z86kBd26",
        "colab_type": "text"
      },
      "source": [
        "Our Neural Network is a single (shallow) hidden layer network defined by three layers:\n",
        "*  Input: profiles embeddings lies into $R^{64}$. This explains the input shape 'shape=(64,)'\n",
        "*  Hidden Layer: a simple 64-neurons dense using tanh ($x\\mapsto (e^x-1)/(e^x+1)$) activation function\n",
        "*  Output: probabilities-like real numbers using softmax activation function.\n",
        "\n",
        "Since we are building a classifier we are compiling with the most common loss and optimizer (categorical crossentropy and Adam respectively). More informations about tensorflow neural network library can be found in https://www.tensorflow.org/api_docs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcpmuheFT4Vx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "model_input = Input(shape=(64,))\n",
        "dense = Dense(64, activation='tanh')(model_input)\n",
        "softmax = Dense(7, activation='softmax')(dense)\n",
        "model = Model(inputs=[model_input], outputs=[softmax])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Urzsu6BJkZK3",
        "colab_type": "text"
      },
      "source": [
        "#### 2.2.b. Train-Test Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0azrH-tqB8Hq",
        "colab_type": "text"
      },
      "source": [
        "Our dataset (1488 items) is splitting into two subsets:\n",
        "*  **Training Set**: 67% of the dataset is used for the model's training phase\n",
        "*  **Validation Set**: 33% of dataset (the remaining part) is used for model validation (using confusion matrix and principal component analysis) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqDyoQAflP47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "jobs_list = list(set(jobs_types))\n",
        "jobs_types_labels = {job_type: index for job_type, index in zip(jobs_list, range(len(jobs_list)))}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9ujf9GekeI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "jobs_embeddings_train, jobs_embeddings_test, jobs_types_train, jobs_types_test, jobs_texts_train, jobs_texts_test = train_test_split(jobs_embeddings, \n",
        "                                                                                                                                     jobs_types, \n",
        "                                                                                                                                     jobs_texts, \n",
        "                                                                                                                                     test_size=0.33)\n",
        "labels_train = [jobs_types_labels[job] for job in jobs_types_train]\n",
        "labels_test = [jobs_types_labels[job] for job in jobs_types_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmwhJN2Slbsz",
        "colab_type": "text"
      },
      "source": [
        "#### 2.2.c. Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FeoZuMmlbCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(x=jobs_embeddings_train, \n",
        "          y=to_categorical(labels_train),\n",
        "          epochs=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1ce5Gh0mIHR",
        "colab_type": "text"
      },
      "source": [
        "#### 2.2.d. Evaluation and Analysis on Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAERlE0J7ZvN",
        "colab_type": "text"
      },
      "source": [
        "The model is evaluated on its validation with two different evaluation methods:\n",
        "\n",
        "1.   **Confusion Matrix**: a matrix that shows the number of:\n",
        "*   On the **diagonal**: **rightfully predicted** classes\n",
        "*   Anywhere else: wrongly classified resumes\n",
        "\n",
        "\n",
        "2.   **Principal Component Analysis Plot**: uses dimension reduction (projection towards high variance axes) to show high dimensional vectors into a lower dimension (usually 2 or 3). Clusters of jobs are showed in the 3-dimensional space\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTZtX2V0mPHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from pandas.core.frame import DataFrame\n",
        "\n",
        "# Scatter Plot Hover Text Formating\n",
        "def line_jump(text, every_char=50):\n",
        "    n_jumps = len(text) // every_char\n",
        "    output = text[:every_char]\n",
        "    for index in range(1, n_jumps):\n",
        "        output += '<br />' + text[every_char*index:every_char*(index+1)] \n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThRQY9_lmHe8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute Model's Predictions on Test Set\n",
        "predictions = np.argmax(model.predict(jobs_embeddings_test), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaJMPr8lmXs3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Confusion Matrix\n",
        "confusion_matrix = tf.math.confusion_matrix(labels_test, predictions)\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "fig, ax = plt.subplots(figsize=(10,10)) \n",
        "sns.heatmap(confusion_matrix, \n",
        "            linewidths=0.5, cmap=\"YlGnBu\", square=True, \n",
        "            xticklabels=jobs_list, yticklabels=jobs_list,\n",
        "            annot=True, fmt='g')\n",
        "plt.xlabel('True Label', fontsize=15)\n",
        "plt.ylabel('Predicted Label', fontsize=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SmGkByNmbsd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Principal Component Analysis in Dimension 3\n",
        "pca = PCA(n_components=3).fit_transform(jobs_embeddings_test)\n",
        "\n",
        "# DataFrame\n",
        "df = DataFrame({'Title': [job['title'] for job in jobs_texts_test],\n",
        "                'Description': [line_jump(job['description'][:1000], 75) for job in jobs_texts_test],\n",
        "                'Predicted Job Type': [jobs_list[pred] for pred in predictions],\n",
        "                'Job Type': jobs_types_test,\n",
        "                'Classification Success': [jobs_list[pred]==job for pred, job in zip(predictions, jobs_types_test)],\n",
        "                'First PCA Axis': pca[:, 0], \n",
        "                'Second PCA Axis': pca[:, 1], \n",
        "                'Third PCA Axis': pca[:, 2]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AreJivDmgok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scatter Plot\n",
        "fig = px.scatter_3d(df, x='First PCA Axis', y='Second PCA Axis', z='Third PCA Axis', \n",
        "                    hover_data=['Description', 'Predicted Job Type', 'Job Type', 'Classification Success'],\n",
        "                    hover_name='Title',\n",
        "                    color='Predicted Job Type',\n",
        "                    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
        "                    symbol='Classification Success',\n",
        "                    symbol_map={True: \"circle\", False: \"square-open\"},\n",
        "                    width=800, height=800, template='plotly_white')\n",
        "fig.update(layout_showlegend=False)\n",
        "fig.update_traces(marker=dict(size=6, line=dict(width=1)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}